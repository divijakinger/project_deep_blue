{"classes":["PERSON","LOCATION","SKILLS","CONTACT","EMAIL","ROLE","ORGANISATION","DURATION","CERTIFICATE","PROJECTS","EDUCATION","INSTITUTION"],"annotations":[["Nagaveena Matta\nSerilingampalle, Andhra Pradesh\n-Email me on Indeed: http://www.indeed.com/r/Nagaveena-Matta/42b26933efde43d6\n\n● Around 9 years of experience in IT Industry using Talend Integration Suite (7.2/6.3/6.1) and\nexperience with Talend Admin Console (TAC) and Datastage. \n● Extensively created mappings in TALEND using tMap, tJoin, tReplicate, tParallelize, tConvertType,\ntFlowToIterate, tAggregate, tSortRow, tFlowMeter, tLogCatcher, tRowGenerator, tNormalize,\ntDenormalize, tHashInput, tHashOutput, tJava, tJavarow, tAggregateRow, tWarn, tLogCatcher,\ntMysqlScd, tFilter, tGlobalmap, tDie, tFTP components, tRESTClient, tRESTRequest, tRESTResponse\netc. \n● Extensive experience in using Talend features such as context variables, triggers, and connectors\nfor Database and flat files like tMySqlInput, tMySqlConnection, tOracle, tMSSQLInput, TMsSqlOutput,\ntMysqlRow, tFileCopy, tfileInputDelimited, tFileExist. \n● Experience working with Data Warehousing Concepts like OLAP, OLTP, Star Schema, Snowflake\nSchema, Logical/Physical/ Dimensional Data Modelling. \n● In depth understanding of the Gap Analysis i.e., As-Is and To-Be business processes and experience\nin converting these requirements into Technical Specifications and Test Plans. \n● Highly Proficient in Agile, Test Driven, Iterative, Scrum and Waterfall software development life\ncycle. \n● Extensively used ETL methodology for performing Data Profiling, Data Migration, Extraction,\nTransformation and Loading using Talend and designed data conversions from wide variety of source\nsystems including Netezza, Oracle, DB2, SQL server, Teradata, Hive non-relational sources like flat\nfiles, XML and Mainframe Files. \n● Extracted data from multiple operational sources for loading staging area, Data warehouse, Data\nMarts using SCDs (Type 1/Type 2/ Type 3) loads. \n● Experience in using cloud components and connectors to make API calls for accessing data from\ncloud storage (Amazon S3) in Talend. \n● Experience in creating Joblets in TALEND for the processes which can be used in most of the jobs in\na project like to Start job and Commit job. \n● Expertise in creating sub jobs in parallel to maximize the performance and reduce overall job\nexecution time with the use of parallelize component of Talend in TIS and using the Multithreaded\nExecutions in Talend. \n● Experienced in creating Triggers on TAC server to schedule Talend jobs to run on server. \n● Strong experience in Extraction, Transformation, loading (ETL) data from various sources into Data\nWarehouses and Data Marts using Informatica Power Centre (Designer, Workflow Manager, Workflow\nMonitor, Metadata Manager). \n● Worked extensively on Error Handling, Performance Analysis and Performance Tuning of Informatica\nETL Components, Teradata Utilities, UNIX Scripts, SQL Scripts etc. \n● Strong decision-making and interpersonal skills with result-oriented dedication towards goals.\n\nWilling to relocate: Anywhere\n\n\fWork Experience\n\nSr. ETL Talend Developer\nINTEGRATION DEVELOPER NETWORK LLC - Philadelphia, PA\nJanuary 2019 to Present\n\nUniversity of Pennsylvania Selected Banner® By Ellucian® to Help Provide Vital Services to Students\nand  University  Community  with  its  \"Next  Generation  Student  Systems\"  initiative.  Modernization\nof  the  university's  systems  will  facilitate  Penn's  ability  to  support  academic  innovation,  including\ninterdisciplinary  and  multiple-degree  programs,  novel  course  types  and  delivery  modes,  and  access\nby  non-traditional  students  of  all  kinds.  It  will  also  help  Penn  maintain  regulatory  compliance  while\nembracing these complex and fast-moving changes.\n\nThe project will replace aging information systems--some more than three decades old--using a phased\napproach. First to be addressed will be student accounts and billing, along with core infrastructure and\nprocesses that support all the systems. Later phases will address student records and registration as\nwell as student financial aid.\n\nResponsibilities\n• Involved in building the ETL architecture and Source to Target mapping to load data into Oracle DB\n(Banner tables).\n• Performed data manipulations using various Talend components like tMap, tJavaRow, tjava, tOracleRow,\ntOracleInput, tOracleOutput, tMSSQLInput and many more.\n• Designed and customized data models for Data warehouse supporting data from multiple sources on\nreal time\n• Designed ETL process using Talend Tool to load from Sources to Targets through data Transformations.\n• Monitored and supported the Talend jobs scheduled through Talend Admin Centre (TAC)\n• Developed the Talend mappings using various transformations, Sessions and Workflows.\n• Used Talend to Extract, Transform and Load data into Netezza Data Warehouse from various sources\nlike Oracle and flat files.\n• Created multiple Joblets (reusable code) & Java routines in Talend.\n• Experienced in writing SQL Queries and used Joins to access data from Oracle, and MySQL.\n• Created mapping documents to outline data flow from sources to targets.\n• Involved in Dimensional modelling (Star Schema) of the Data warehouse and used Erwin to design the\nbusiness process, dimensions and measured facts.\n•  Responsible  for  developing,  support  and  maintenance  for  the  ETL  (Extract,  Transform  and  Load)\nprocesses using Talend.\n• Developed Talend ESB services and deployed them on ESB servers on different instances using tRest\nand ESB components.\n• Unit testing, code reviewing, moving in UAT and PROD.\n• Designed the Talend ETL flow to load the data into hive tables and create the Talend jobs to load the\ndata into Oracle and Hive tables.\n•  Working  with  high  volume  of  data  and  tracking  the  performance  analysis  on  Talend  job  runs  and\nsession.\n• Prepare the Talend job level LLD documents and working with the modelling team to understand the\nBig Data Hive table structure and physical design.\n• Conducted code reviews developed by my teammates before moving the code into QA.\n• Used Talend reusable components like routines, context variable and globalMap variables.\n\n\f• Modified existing mappings for enhancements of new business requirements.\n•  Prepared  migration  document  to  move  the  mappings  from  development  to  testing  and  then  to\nproduction repositories.\n•  Works  as  a  fully  contributing  team  member,  under  broad  guidance  with  independent  planning  &\nexecution responsibilities.\nEnvironment: Talend Open studio V(6.1.1)Enterprise Platform for Data management (V6.1.1,5.5.1, 5.6.1),\nUNIX, Hadoop, Oracle, Microsoft SQL Server management Studio.\n\nSr. ETL Talend Developer\nINTEGRATION DEVELOPER NETWORK LLC\nFebruary 2015 to Present\n\nETL Talend Developer\nINTEGRATION DEVELOPER NETWORK LLC\nJanuary 2018 to December 2018\n\nIRecord is a Managed Healthcare System. Provides services for people with disabilities and are over the\nage of 21.\n\nResponsibilities\n•  Involved  in  building  the  ETL  architecture  and  Source  to  Target  mapping  to  load  data  into  Data\nwarehouse.\n• Created Talend jobs to copy the files from one server to another and utilized Talend FTP components.\n• Performed data manipulations using various Talend components like tMap, tJavaRow, tjava, tOracleRow,\ntOracleInput, tOracleOutput, tMSSQLInput and many more.\n• Created Talend jobs using the dynamic schema feature.\n• Designed ETL process using Talend Tool to load from Sources to Targets through data Transformations.\n• Developed complex Talend ETL jobs to migrate the data from flat files to database.\n•  Responsible  for  developing,  support  and  maintenance  for  the  ETL  (Extract,  Transform  and  Load)\nprocesses using Talend.\n•  Worked  on  different  tasks  in  Workflows  like  sessions,  events  raise,  event  wait,  decision,  e-mail,\ncommand, worklets, Assignment, Timer and scheduling of the workflow.\n•  Working  with  high  volume  of  data  and  tracking  the  performance  analysis  on  Talend  job  runs  and\nsession.\n• Implemented custom error handling in Talend jobs and worked on different methods of logging.\n• Conducted code reviews developed by my teammates before moving the code into QA.\n• Experience in Batch scripting on windows, Windows 32-bit commands, Quoting, Escaping.\n• Used Talend reusable components like routines, context variable and globalMap variables.\n• Modified existing mappings for enhancements of new business requirements.\n•  Prepared  migration  document  to  move  the  mappings  from  development  to  testing  and  then  to\nproduction repositories.\n• Experienced in writing SQL Queries and used Joins to access data from Oracle, and MySQL.\n• Implemented Error Logging, Error Recovery, and Performance Enhancement's & created Audit Process\n(generic) for various Application teams.\n•  Works  as  a  fully  contributing  team  member,  under  broad  guidance  with  independent  planning  &\nexecution responsibilities.\n• Prepared ETL mapping Documents for every mapping and Data Migration document for smooth transfer\nof project from development to testing environment and then to production environment.\n\n\fEnvironment: Talend Open studio V(6.1.1)Enterprise Platform for Data management (V6.1.1,5.5.1, 5.6.1),\nUNIX, Hadoop, Oracle, Microsoft SQL Server management Studio.\n\nETL Talend Developer\nINTEGRATION DEVELOPER NETWORK LLC - Hartford, CT\nMarch 2016 to December 2017\n\nNassau  Re  is  headquartered  in  Hartford,  Connecticut,  and  has  three  primary  insurance  company\noperating  subsidiaries,  Nassau  Life  and  Annuity  Company,  Nassau  Life  Insurance  Company  and  PHL\nVariable  Insurance  Company.  These  insurance  companies  were  part  of  The  Phoenix  Companies,  Inc.,\nwhich Nassau Re acquired in 2016. Since then, Nassau Re contributed significant new equity capital to\nimprove financial strength and liquidity.\n\nResponsibilities:\n• Worked with Data mapping team to understand the source to target mapping rules.\n• Analysed the requirements and framed the business logic and implemented it using Talend. Involved\nin ETL design and documentation.\n• Analysed and performed data integration using Talend open integration suite. Worked on the design,\ndevelopment and testing of Talend mappings.\n• Created ETL job infrastructure using Talend Open Studio.\n• Worked on Talend components like tReplace, tmap, tsort and tFilterColumn, tFilterRow, tJava, Tjavarow,\ntConvertType etc.\n• Used Database components like tMSSQLInput, tMsSqlRow, tMsSqlOutput, tOracleOutput, tOracleInput\netc.\n• Worked with various File components like tFileCopy, tFileCompare, tFileExist, TFileDelete, tFileRename.\n• Worked on improving the performance of Talend jobs.\n• Created triggers for a Talend job to run automatically on the server. Worked on Exporting and Importing\nTalend jobs.\n• Created jobs to pass parameters from child job to parent job.Exported jobs to Nexus and SVN repository.\n• Implemented update strategy on tables and used tJava, tJavarow components to read data from tables\nto pull only newly inserted data from source tables.\n• Observed statistics of Talend jobs in AMC to improve the performance and in what scenarios errors are\ncausing Created Generic and Repository schemas.\n• Developed project specific 'Deployment' job responsible to deploy Talend jar files on to the windows\nenvironment as a zip file, later, this zip file is unzipped and the files are again deployed to the UNIX box.\n• Also, this deployment job is responsible to maintain versioning of the Talend jobs that are deployed\nin the UNIX environment.\n• Developed shell scripts in UNIX environment to support scheduling of the Talend jobs.\n• Monitored the daily runs, weekly runs and adhoc runs to load data into the target systems.\nEnvironment:  Talend  6.1/5.5.2,  UNIX,  Shell  script,  SQL  Server,  Oracle,  Business  Objects,  ERwin,  SVN,\nRedgate, Capterra.\n\nDatastage Developer\nINTEGRATION DEVELOPER NETWORK LLC - Dayton, OH\nFebruary 2015 to February 2016\n\nResponsibilities:\n• Designed the ETL jobs using IBM Info sphere Data Stage 9.1 to Extract, Transform and load the data\ninto staging, ODS and EDW.\n\n\f•  Designed  and  developed  the  ETL  jobs  using  Parallel  Edition  which  distributed  the  incoming  data\nconcurrently across all the processors, to achieve the best performance.\n•  Designed  parallel  jobs  using  stages  such  as  Join,  Merge,  Lookup,  Remove  Duplicates,  Copy,  Filter,\nFunnel, Dataset, Lookup, Pivot, and Sort, Surrogate key Generator, Change Data Capture (CDC), Modify,\nRow Generator and Aggregator.\n• Responsible for generation of DDL statements which are executed for database creation.\n• Responsible for preparing Physical/logical data models.\n•  Responsible  for  data  analysis,  requirements  gathering,  report  analysis,  source-to-target  mapping,\nfrequency analysis, process flow diagrams, and documentation.\n• Handled Performance Tuning of Jobs to ensure faster Data Loads.\n• Designed sequence jobs using the activities such as Job Activity, Nested Condition, Notification Activity,\nSequencer Activity, Terminator Activity and Execute Command.\n• Performed the Integration and System testing on the ETL jobs. Responsible for preparing ad hoc jobs.\n• Translated business rules and functionality requirements into ETL derivations.\n• Scheduled jobs using Autosys scheduler utility based on the requirements and monitored the production\nprocesses closely for any possible errors.\n•  Migrated  projects  from  development  to  QA  to  Production  environments.  Assisted  operation  support\nteam for transactional data loads in developing SQL&UNIX scripts.\n• Imported the required Metadata from heterogeneous sources at the process level.\n•  Created  Job  Parameters  and  Environment  variables  to  run  the  same  job  for  different  sources  and\ntargets.\n• Used Multi-job-compiler during deployment of projects.\n• Created Batches (DS job controls) and Sequences to control set of jobs.\n• Used the Director to schedule running the job, testing and debugging its components, and monitoring.\nCreated Shared Containers for Re-using the Business functionality.\n• Collaborated with BO team to design Crystal reporting and reports for enterprise reporting applications.\n• Worked with Developers to troubleshoot and resolve issues in job logic as well as performance.\n\nEnvironment: ETL DataStage, DBW, Microsoft SQL 2005/2008, Netezza, IBM DB2 9.1, AIX6.0, Oracle and\nUNIX.\n\nETL Talend Developer\nDeloitte - Hyderabad, Telangana\nDecember 2011 to January 2014\n\nResponsibilities:\n● Worked closely with Business Analysts to review the business specifications of the project and also\nto gather the ETL requirements.\n●  Developed  jobs,  components  and  Joblets  in  Talend.  Designed  ETL  Jobs/Packages  using  Talend\nIntegration Suite (TIS)\n●  Created  complex  mappings \ntPivotToColumnsDelimited as well as custom components such as tUnpivotRow.\n● Used tStatsCatcher, tDie, tLogRow to create a generic joblet to store processing stats into a Database\ntable to record job history.\n● Created Talend Mappings to populate the data into dimensions and fact tables. Frequently used Talend\nAdministrative Console (TAC)\n● Implemented new users, projects, tasks within multiple different environments of TAC (Dev, Test, Prod,\nand DR).\n\nin  Talend  using \n\ntDenormalize, \n\ntUniqueRow.\n\ntHash, \n\ntMap, \n\n\f●  Developed  complex  Talend  ETL  jobs  to  migrate  the  data  from  flat  files  to  database.  Implemented\ncustom  error  handling  in  Talend  jobs  and  also  worked  on  different  methods  of  logging.  Created  ETL/\nTalend jobs both design and code to process data to target databases.\n●  Created  Talend  jobs  to  load  data  into  various  Oracle  tables.  Utilized  Oracle  stored  procedures  and\nwrote fewJava code to capture global map variables and use them in the job.\n●  Successfully  Loaded  Data  into  different  targets  from  various  source  systems  like  Oracle  Database,\nDB2, Flatfiles, XML files etc into the Staging table and then to the target database.\n● Troubleshoot long running jobs and fixing the issues.\n● Prepared ETL mapping Documents for every mapping and Data Migration document for smooth transfer\nof project from development to testing environment and then to production environment. Performed Unit\ntesting and System testing to validate data loads in the target.\n\n• Talend Integration Suite (7.2/6.3/6.1/5.x) / Talend Open Studio (7.2/6.1/5.x) and experience with\n\nEducation\n\nB.tech in Computer science engineering\nJNTU university\n\nSkills / IT Skills\n\n• ETL Talend\n\nTalend Admin Console (TAC)\n\n• Languages\n\n• JAVA, C#, HTML, XML, PL/SQL\n\n• Data warehouse\n\n• Datastage, Informatica, SSIS, SSRS\n\n• Databases\n\n• Build Tool\n\n• Ant, Maven\n\n• Processes\n\n• Agile-Scrum, Waterfall\n\n• Operating Systems\n\n• Windows, Linux, Mac\n\n• Oracle, SQL Server, Teradata, Hadoop, HDFS, MongoDB\n\n• Web Debugging Tool XPath, Firepath, Firebug\n\n• Bug Tracking Tools JIRA, Quality Centre (QC) and BugZilla\n\n\f",{"entities":[[0,15,"PERSON"],[33,47,"LOCATION"],[136,143,"DURATION"],[179,203,"SKILLS"],[238,258,"SKILLS"],[260,263,"SKILLS"],[269,279,"SKILLS"],[2957,2981,"ROLE"],[2982,3015,"ORGANISATION"],[3018,3030,"LOCATION"],[3032,3034,"LOCATION"],[3035,3058,"DURATION"],[3060,3086,"INSTITUTION"],[6431,6451,"SKILLS"],[6458,6477,"SKILLS"],[6521,6525,"SKILLS"],[6527,6533,"SKILLS"],[6535,6541,"SKILLS"],[6553,6556,"SKILLS"],[6584,6608,"ROLE"],[6609,6642,"ORGANISATION"],[6643,6667,"DURATION"],[6669,6689,"ROLE"],[6690,6723,"ORGANISATION"],[6724,6753,"DURATION"],[11692,11698,"SKILLS"],[11712,11716,"SKILLS"],[11719,11732,"SKILLS"],[11735,11746,"SKILLS"],[11749,11755,"SKILLS"],[11758,11775,"SKILLS"],[11778,11783,"SKILLS"],[11786,11789,"SKILLS"],[11791,11798,"SKILLS"],[11800,11809,"SKILLS"],[11811,11830,"ROLE"],[11831,11864,"ORGANISATION"],[11867,11873,"LOCATION"],[11875,11877,"LOCATION"],[11878,11908,"DURATION"],[14365,14385,"ROLE"],[14386,14394,"ORGANISATION"],[14397,14406,"LOCATION"],[14408,14417,"LOCATION"],[14418,14447,"DURATION"],[16405,16443,"EDUCATION"],[16444,16459,"INSTITUTION"],[16483,16493,"SKILLS"],[16538,16542,"SKILLS"],[16544,16546,"SKILLS"],[16548,16552,"SKILLS"],[16554,16557,"SKILLS"],[16559,16565,"SKILLS"],[16569,16583,"SKILLS"],[16587,16596,"SKILLS"],[16598,16609,"SKILLS"],[16611,16615,"SKILLS"],[16617,16621,"SKILLS"],[16625,16634,"SKILLS"],[16638,16648,"SKILLS"],[16652,16655,"SKILLS"],[16657,16662,"SKILLS"],[16666,16675,"SKILLS"],[16679,16690,"SKILLS"],[16692,16701,"SKILLS"],[16726,16733,"SKILLS"],[16735,16740,"SKILLS"],[16742,16745,"SKILLS"],[16749,16755,"SKILLS"],[16757,16760,"SKILLS"],[16769,16777,"SKILLS"],[16779,16785,"SKILLS"],[16787,16791,"SKILLS"],[16793,16800,"SKILLS"],[16823,16828,"SKILLS"],[16830,16838,"SKILLS"],[16840,16847,"SKILLS"],[16870,16874,"SKILLS"],[16876,16890,"SKILLS"],[16892,16894,"SKILLS"],[16900,16908,"SKILLS"]]}]]}